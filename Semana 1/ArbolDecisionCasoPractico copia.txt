Detalle del ejemplo práctico de árboles de decisión
Contexto:
Una Empresa del sector financiero desea implementar una estrategia de marketing que permita comunicar los cambios de política en uno de los beneficios que tiene su tarjeta de crédito. Actualmente, todos Bancos de la región se encuentran implementando políticas similares, por lo que es importante evaluar si esas políticas percudirán a sus clientes más valiosos. La hipótesis que se tiene es que como el beneficio en cuestión no tiene restricción, es usado por los clientes menos valiosos, y esto hace que mantenerlo no sea rentable.

Objetivo
La institución solicita un análisis al área de datos con la finalidad de entender y analizar los clientes que han accedido al beneficio y el comportamiento que han tenido. Y de igual forma, proponer un escenario en el que se pueda restringir el uso y que clientes serían los afectados.

Descripción Data Set a usar
Los datos entregados tienen relación al uso de los clientes en cuanto a un beneficio otorgado por el uso de su tarjeta de crédito.
* **ClienteId:**  Identificador único del cliente.
* **CostoEntrada:** Costo total que tiene el uso del beneficio.
* **Cupo:** Cupo de la tarjeta de crédito.
* **EntradasMensuales:**  Número de veces que ha accedido al beneficio.
* **Año:**  Año de acceso del beneficio.
* **Mes:**  Mes de acceso del beneficio.
* **PerfilIngreso:**  Tipo de cliente según la institución.
* **PromedioConsumoMensual:** Promedio Consumo Mensual que ha tenido el cliente en su Tarjeta de crédito.
* **Utilidad Financiera:**  Ingreso que el cliente ha dejado a la institución luego de restar los costos. 
* **SaldoCapital:** Valor de los consumos que tiene el cliente en su tarjeta de crédito en el momento de usar su beneficio.
Recuerda que en la vida práctica no se tienen las definiciones de cada una de las variables descritas en el problema, es muy común tener que acceder a ellas a través del área de Gobierno de Datos y/o los Data Stewards de negocio que son dueños de cada concepto.

**Codigo
%pip install openpyxl
%pip install prince
%pip install --upgrade typing_extensions

dbutils.library.restartPython()
#Realizamos la importación de librería necesarias

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.sql.functions import col, max as spark_max, expr
from pyspark.sql.window import Window
from pyspark.sql.functions import ntile
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.tree import plot_tree
from sklearn.model_selection import ParameterGrid
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder
from prince import FAMD
from sklearn.decomposition import PCA
from pyspark.sql.functions import when, col, count, lit, min, max,round,concat_ws

#Leemos el archivo que se tiene como input, de igual forma se comprueba que se ha importado de manera correcta
df = pd.read_excel('/Workspace/Users/mosquerafr@produbanco.com/dataAD.xlsx')
df.head(5)

# Realizamos un descriptivo de los principales indicadores estadísticos para las variables que son continuas
df1 = df.drop(columns=['Año'])
df1.describe().round(2)

%python
#Valores distintos para cada Cliente ID
distinct_count = df['ClienteId'].drop_duplicates().count()
distinct_count

Al verificar los estadísticos de las variables que son continuas en el Dataset podemos observar, que existen **108,642** observaciones que se han entregado. Al observar la estructura de la tabla, vemos que se han entregado más de un registro por cliente. En la celda anterior, al hacer este ejercicio vemos que solo tenemos **33961 clientes** que han realizado 108,642 usos del beneficio.

Al analizar de igual forma la Data por cada año, vemos que tenemos observaciones desde el 2022 hasta el 2024. Es importante mencionar que tenemos **dos años completos (2022 - 2023) y un año parcial (2024)**.

distinct_df = df[['Año', 'Mes']].drop_duplicates().sort_values(by='Año')
display(distinct_df)

grouped_df = distinct_df.groupby('Año').size().reset_index(name='count')
display(grouped_df)

import matplotlib.pyplot as plt

# Drop the 'ClienteId' column
df1_filtered = df1.drop(columns=['ClienteId'])

# Generate box plots for the remaining variables
df1_filtered.plot(kind='box', subplots=True, layout=(int(len(df1_filtered.columns) / 2), 2), figsize=(12, 8), sharex=False, sharey=False)
plt.tight_layout()
plt.show()

Ahora considerando que los clientes tienen diferentes comportamientos de acuerdo a su realidad económica, se usa la variable perfil ingreso para repetir el ejercicio y verificar si existe comportamiento de valores atípicos y extremos. Como vemos en los gráficos inferiores, la tendencia se mantiene. Por lo que vamos a tener que retirar estas observaciones.


import matplotlib.pyplot as plt

# Drop the 'ClienteId' column
df1_filtered = df1.drop(columns=['ClienteId'])

# Generate box plots for continuous variables by 'PerfilIngreso' category
continuous_vars = df1_filtered.select_dtypes(include=['float64', 'int64']).columns
for var in continuous_vars:
    plt.figure(figsize=(10, 6))
    df1_filtered.boxplot(column=var, by='PerfilIngreso')
    plt.title(f'Box plot of {var} by PerfilIngreso')
    plt.suptitle('')
    plt.xlabel('PerfilIngreso')
    plt.ylabel(var)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

Si procedemos a realizar el ejercicio de quitar estos valores extremos tomando como definición de extremo aquel que sobrepasa 3 veces la distancia intercuatílica, tenemos un comportamiento mucho más normal para poder entender el comportamiento de uso del beneficio en los clientes.

def remove_outliers(df, group_col, value_cols):
    def outlier_bounds(series):
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 3 * IQR
        upper_bound = Q3 + 3 * IQR
        return lower_bound, upper_bound

    def filter_outliers(group):
        for col in value_cols:
            lower_bound, upper_bound = outlier_bounds(group[col])
            group = group[(group[col] >= lower_bound) & (group[col] <= upper_bound)]
        return group

    return df.groupby(group_col).apply(filter_outliers).reset_index(drop=True)


A diferencia de los diagramas de cajas y bigotes anteriores, se puede observar un comportamiento mucho más acotado a la realidad. Es importante observar que los clientes de mejor perfil tienden a tener mejores comportamientos en consumo, y cupo. Sin embargo, en uso del beneficio al parecer todos usan de la misma manera.

import matplotlib.pyplot as plt

# Drop the 'ClienteId' and 'Año' columns
df_cleaned_filtered = df_cleaned.drop(columns=['ClienteId', 'Año'])

# Generate box plots for continuous variables by 'PerfilIngreso' category
continuous_vars = df_cleaned_filtered.select_dtypes(include=['float64', 'int64']).columns
for var in continuous_vars:
    plt.figure(figsize=(5, 2))
    df_cleaned_filtered.boxplot(column=var, by='PerfilIngreso')
    plt.title(f'Box plot of {var} by PerfilIngreso')
    plt.suptitle('')
    plt.xlabel('PerfilIngreso')
    plt.ylabel(var)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

De igual forma si observamos las estadísticas principales para las variables analizadas, y las comparamos con las iniciales. Podemos ver un comportamiento mucha más normal en cuanto a máximos y mínimos.

df_cleaned.describe()
df.describe().round(2)

Una vez que hemos realizado la depuración de valores atípicos, procedemos a simplificar la correlación de las variables a través de un ACP, por lo que preparamos todo para que se pueda implementar como sigue:

Un ACP (Análisis de Componentes Principales), recibe variables numéricas, por lo que procedemos a realizar algunas transformaciones que permitan el uso de la función. Si no estas familiarizado con esta ténica, te dejo un link para puedas profundizar si lo necesitas. En resumen, permite consolodirar en variables numpericas denominadas componentes principlaes los efectos de más variables. En este caso, nuestra intención es poder resumir los efectos de las variables analizadas para generar una componente que permita identificar de mejor manera los mejores clientes.

def recategorizar_perfil_ingreso(producto):
    producto = producto.lower()
    if 'cluster 1' in producto:
        return  0
    elif '2' in producto:
        return 500
    elif '3' in producto:
        return 1000
    elif 'prefer' in producto:
        return 2500
    elif 'plus' in producto:
        return 6000


# Aplicar la función a la columna 'SubProductoActivo'
df_cleaned['PerfilIngreso_cod'] = df_cleaned['PerfilIngreso'].apply(recategorizar_perfil_ingreso)
print(df_cleaned['PerfilIngreso_cod'] )

df_cleaned.groupby(['PerfilIngreso','PerfilIngreso_cod']).size()

#Obtenemos los nombres de variables en vectores para fácil manipulación.
num = ['PromedioConsumoMensual','EntradasMensuales','Cupo','Utilidad Financiera','SaldoCapital']
#cagnom=['Producto_cat']
cagord=['PerfilIngreso_cod']

Hay que recordar que pese a que las variables númericas pueden procesarse en un ACP, siempre es importante que se encuentren en la misma escala. Por lo que procedemos a escalar para que se puedan interpretar sin generar sesgos por la forma en la que se miden. Por ejemplo si comparamos entre el Monto Mensual Consumo y el Número de entradas, claramente un monto de $1000 será mucho más granda que 3 Entradas. Con el escaldo, lo que hacemos es transformarlas para que adimensionalmente se pueda comprar alguiente que usa mucho el beneficio vs alguien que consumo bastante.

# Columnas: num (variables numéricas), cagnom (variables categóricas nominales), cagord (variables categóricas ordinales)
# Paso 1: Preparación de datos
# Escalado de variables numéricas
scaler = StandardScaler()
dfb = df_cleaned.copy()
dfb = dfb.dropna(subset=num + cagord)

dfb[num+ cagord] = scaler.fit_transform(dfb[num+ cagord])

# One-Hot Encoding para variables categóricas nominales
#encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
#encoded_nom = encoder.fit_transform(dfb[cagnom ])
#names_nom = encoder.get_feature_names_out(cagnom)

# Conversión a DataFrame para facilitar la integración
#encoded_nom_df = pd.DataFrame(encoded_nom, columns=names_nom, index=dfb.index)

# Concatenar variables numéricas, ordinales y codificadas nominales
prepared_df = pd.concat([dfb[num+ cagord]], axis=1)

pca_al = PCA()
pca_resultados = pca_al.fit_transform(prepared_df)

# Crear un DataFrame con los resultados de los componentes principales
columns = [f'PC{i+1}' for i in range(pca_al.n_components_)]
pca_df = pd.DataFrame(pca_resultados, columns=columns, index=dfb.index)

# Porcentaje de varianza explicada de cada componente
explained_variance_ratio = pca_al.explained_variance_ratio_
explained_variance_df = pd.DataFrame({
    'Componente Principal': columns,
    'Porcentaje de Varianza Explicada': explained_variance_ratio * 100
})

display(explained_variance_df)

# Paso 3: Visualización de resultados
# Varianza explicada acumulada
def plot_variance_explained(pca_al):
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, len(pca_al.explained_variance_ratio_)+1), 
             pca_al.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')
    plt.title('Varianza explicada acumulada')
    plt.xlabel('Número de Componentes Principales')
    plt.ylabel('Varianza Explicada Acumulada')
    plt.grid()
    plt.show()

# Gráfico de las dos primeras componentes principales
def plot_pca_scatter(pca_df):
    plt.figure(figsize=(8, 5))
    plt.scatter(pca_df['PC1'], pca_df['PC2'], alpha=0.5)
    plt.title('Scatter Plot de las dos primeras componentes principales')
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.grid()
    plt.show()


# Eigenvalores y porcentaje de varianza explicada
eigenvalues = pca_al.explained_variance_
explained_variance_ratio = pca_al.explained_variance_ratio_
cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

print("\nEigenvalores (varianza explicada):", eigenvalues)
print("\nProporción de varianza explicada:", explained_variance_ratio)
print("\nVarianza acumulada:", cumulative_variance_ratio)

# Crear un DataFrame para las proporciones de varianza
componentes_principales = [f"PCA{i+1}" for i in range(len(explained_variance_ratio))]
variance_df = pd.DataFrame({
    "Componente Principal": componentes_principales,
    "Varianza Explicada (%)": explained_variance_ratio * 100,
    "Varianza Acumulada (%)": cumulative_variance_ratio * 100
})

print("\nResumen de la varianza explicada:")
print(variance_df)

# Gráficos y análisis

# 1. Scree Plot (Eigenvalores)
plt.figure(figsize=(8, 6))
plt.bar(range(1, len(eigenvalues) + 1), eigenvalues, tick_label=componentes_principales)
plt.title("Scree Plot (Eigenvalores)")
plt.xlabel("Componentes Principales")
plt.ylabel("Eigenvalores")
plt.show()

# 2. Varianza Explicada (Gráfico de Barras)
plt.figure(figsize=(8, 6))
plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, tick_label=componentes_principales)
plt.title("Proporción de Varianza Explicada")
plt.xlabel("Componentes Principales")
plt.ylabel("Proporción de Varianza")
plt.show()

# 3. Gráfico de cómo se relacionan los componentes con las variables originales
def plot_pca_loadings(pca, features):
    plt.figure(figsize=(10, 8))
    plt.quiver(np.zeros(pca.components_.shape[1]), np.zeros(pca.components_.shape[1]), 
               pca.components_[0, :], pca.components_[1, :], 
               angles='xy', scale_units='xy', scale=1, color='r')
    for i, feature in enumerate(features):
        plt.text(pca.components_[0, i] * 1.15, pca.components_[1, i] * 1.15, 
                 feature, color='g', ha='center', va='center')
    plt.xlim(-1, 1)
    plt.ylim(-1, 1)
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.title('Relación de las variables originales con los componentes principales   ')
    plt.grid()
    plt.axhline(0, color='black',linewidth=0.5)
    plt.axvline(0, color='black',linewidth=0.5)
    plt.show()

loadings = pd.DataFrame(pca_al.components_.T, columns=columns, index=prepared_df.columns)

suma_columnas = loadings.abs().sum() # Calcular el porcentaje de cada valor en las columnas
porcentaje_columnas = loadings.div(suma_columnas, axis=1).abs()  # Ver el resultado 
porcentaje_columnas.style.format('{:.0%}')

for i in range(5):
    df_cleaned[f'PCA{i+1}'] = pca_resultados[:, i]

**Análisis de Entradas**

meses_mapping = {
    'enero': 1, 'febrero': 2, 'marzo': 3, 'abril': 4, 'mayo': 5, 'junio': 6,
    'julio': 7, 'agosto': 8, 'septiembre': 9, 'octubre': 10, 'noviembre': 11, 'diciembre': 12
}
df_cleaned['Mes'] = df_cleaned['Mes'].map(meses_mapping)

df_cleaned = df_cleaned.sort_values(by=['ClienteId', 'Año', 'Mes'], ascending=True)
df_cleaned['Entradas_Acumuladas_Anual'] = df_cleaned.groupby(['ClienteId', 'Año'])['EntradasMensuales'].cumsum()

display(df_cleaned)

# Definir los límites manuales para la agrupación
bins = [1, 2, 3, 4, 5, 6, 8, 12, 18, 30, 200]
labels = ['1', '2', '3', '4', '5', '6-7', '8-11', '12-17', '18-29','30mas']

# Crear la nueva variable 'Entradas_Acumuladas_Anual_Grupo' en el DataFrame dfb
df_cleaned['Entradas_Acumuladas_Anual_Grupo'] = pd.cut(df_cleaned['Entradas_Acumuladas_Anual'], bins=bins, labels=labels, right=False)

display(df_cleaned.head())

#Comprobando que todo esta OK
df_cleaned.groupby("Entradas_Acumuladas_Anual_Grupo")["Entradas_Acumuladas_Anual"].agg(['min', 'max'])

# Crear percentiles para PCA1 y PCA2
df_cleaned['PCA1_percentile'] = pd.qcut(df_cleaned['PCA1'], 10, labels=False)
df_cleaned['PCA2_percentile'] = pd.qcut(df_cleaned['PCA2'], 10, labels=False)

display(df_cleaned.head())


# Create a pivot table for the heatmap Número
pivot_table = df_cleaned.pivot_table(index='Entradas_Acumuladas_Anual_Grupo', columns='PCA2_percentile', aggfunc='size', fill_value=0)

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(pivot_table, annot=True, fmt="d", cmap="YlGnBu")
plt.title('Heatmap of Entradas Anuales and Rango')
plt.xlabel('Rango')
plt.ylabel('Entradas Anuales')
plt.show()


pivot_table = df_cleaned.pivot_table(
    index='Entradas_Acumuladas_Anual_Grupo', 
    columns='PCA2_percentile', 
    values='CostoEntrada', 
    aggfunc='sum', 
    fill_value=0
)

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(pivot_table, annot=True, fmt=".2f", cmap="YlGnBu", cbar_kws={'label': 'Suma de CostoEntrada'})
plt.title('Heatmap of Entradas Anuales and Rango (Costo Total)')
plt.xlabel('Rango')
plt.ylabel('Entradas Anuales')
plt.show()


#Crear el otro heatma para costo

pivot_table = df_cleaned.pivot_table(
    index='Entradas_Acumuladas_Anual_Grupo', 
    columns='PCA2_percentile', 
    values='CostoEntrada', 
    aggfunc='sum', 
    fill_value=0
)

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(pivot_table, annot=True, fmt=".2f", cmap="YlGnBu", cbar_kws={'label': 'Suma de CostoEntrada'})
plt.title('Heatmap of Entradas Anuales and Rango (Costo Total)')
plt.xlabel('Rango')
plt.ylabel('Entradas Anuales')
plt.show()


df_cleaned['Restriccion1'] = ((df_cleaned['PCA2'] > 3) & (df_cleaned['Entradas_Acumuladas_Anual'] < 8)).astype(int)


dfb_grouped1 = df_cleaned.groupby(['Restriccion1', 'Año']).agg(
    total_filas=('ClienteId', 'size'),
    total_clientes_unicos=('ClienteId', 'nunique'),
    suma_CostoEntrada=('CostoEntrada', 'sum'),
    suma_Entradas_Acumuladas_Anual=('EntradasMensuales', 'sum')
).reset_index()
dfb_grouped1

